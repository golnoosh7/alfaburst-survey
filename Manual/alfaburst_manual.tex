\documentclass{article}

\usepackage[a4paper, margin=1in]{geometry}
\usepackage[obeyspaces]{url}
\usepackage{graphicx}

\title{ALFABURST User's Guide}
\author{Jayanth Chennamangalam}

\begin{document}
\maketitle

\tableofcontents

\section{Computers}

You need to have an NAIC account to log in remotely. Once you have logged on to
\url{remoto.naic.edu} using your NAIC account, you can access the ALFABURST
(`AB' from hereon) machines as follows: The head node host name is
\url{alfaburst}. You need to log in to this machine using your AB credentials.
Once you are on the head node, you can access the four compute nodes,
\url{abc0}, \url{abc1}, \url{abc2}, and \url{abc3}.

The compute nodes are PXE-booted off the head node. On the head node, the
directory \url{/srv/precise_root.x86_64} contains this filesystem. In addition,
each compute node has two RAID arrays for data: \url{/dev/md0} is mounted as
\url{/data} and \url{/dev/md1} is mounted as \url{/databk}.


\section{Code}

The AB codebase is split across two repositories:
\url{github.com/jayanthc/pelican-alfaburst} for the data acquisition pipeline,
and \url{github.com/jayanthc/alfaburst-survey} for the survey-related
configuration files and scripts. These are also installed on the head node
(\url{alfaburst}) in \url{/home/artemis/Code/alfaburst/pelican-alfaburst} and
\url{/home/artemis/Survey}, respectively.

\subsection{Data Acquisition Pipeline}

\url{/home/artemis/Code/alfaburst} contains both the PELICAN framework and the AB
code, along with a few files containing the compilation and installation
commands. These are listed below, with the filenames in italics being the ones
to use for production:

\begin{itemize}
\item \url{cmake_pelican}: PELICAN; Default.
\item \emph{\url{cmake_pelican_icpc}}: PELICAN; Intel compiler-based, with
optimization flags.
\item \url{cmake_pelican_debug}: PELICAN; Default, debug mode.
\item \url{cmake_pelican-alfaburst}: AB; Default.
\item \emph{\url{cmake_pelican-alfaburst_icpc}}: AB; Intel compiler-based,
with optimization flags.
\item \url{cmake_pelican-alfaburst_icpc_timing}: AB; Intel compiler-based, with
optimization flags, with \url{TIMING_ENABLED} turned on.
\end{itemize}

Note that, for \url{pelican-alfaburst}, you may need to update the path to the
CUDA library, if you wish to use a newer version of CUDA.

To do a clean install on \url{alfaburst}, login as user \url{artemis}, and do
the following:

\begin{enumerate}
\item \url{cd /home/artemis/Code/alfaburst/pelican/build; rm -rf *}
\item \url{source ../../cmake_pelican_icpc}
\item \url{cd /home/artemis/Code/alfaburst/pelican-alfaburst/build; rm -rf *}
\item \url{source ../../cmake_pelican-alfaburst_icpc}
\end{enumerate}

PELICAN libraries are installed in \url{/home/artemis/Code/alfaburst/pelican/install},
while the AB binaries are installed in \url{/usr/local/pelican-lofar/bin}.

\subsection{Survey Configuration Files and Scripts}

These are configuration files for the pipeline, scripts that control when the
pipeline is run, generate plots after the run, etc. The directory structure is
as follows:

\begin{itemize}
\item \url{Config}: Contains the XML configuration files, with self-explanatory
names, for example, \url{Beam0_server.xml}, \url{Beam0_client.xml}, etc.
\url{alfa.bp} is the bandpass file.
\item \url{Data/Latest}: This is a staging area for making plots. The latest
\url{*dm*} files are copied here from all compute nodes and the plotting script
is run on them.
\item \url{Images}: Contains logos.
\item \url{Log}: Observation logs generated by the survey scripts in
\url{Scripts}.
\item \url{Notes}: Notes on observations, if any.
\item \url{Plots}: Generated plots.
\item \url{Scripts}: Observation scripts.
\item \url{www}: Web pages generated by the plotting scripts.
\end{itemize}


\subsubsection{Configuration}

The beam-to-backend mapping is as follows:

\begin{itemize}
\item \url{abc0}: Beams 0 and 1
\item \url{abc1}: Beams 2 and 3
\item \url{abc2}: Beams 4 and 5
\item \url{abc3}: Beam 6
\end{itemize}

The FPGA sends out UDP packets addressed to ports 16704 (for even-numbered
beams) and 16705 (for odd-numbered beams). These are set in the server
configuration files.

The bandpass file is \url{alfa.bp}, and is currently set up for 1024 channels
(the pipeline extracts the shape of the appropriate 512 channels from this). An
attempt is made to flatten the bandpass, but based on the value of the LO, this
yields success to varying degrees. \url{common.xml} is used by the SIGPROC
writer.


\subsubsection{Data}

The output data consists of two kinds of files - candidate list files and
filterbank files. Candidate list files are written to
\url{/data/Survey/Data/BeamX_dm*} on the compute nodes, where \url{X} is the
beam identifier. A candidate list file (or `\url{*dm*} file') is a text file
that contains comma-separated values, namely, MJD, DM and S/N of the event, and
the smoothing length corresponding to the detection. The filterbank files are
written to \url{/data/Survey/Data/BeamX_fb*} on the compute nodes. These files
contain the buffers that events were detected in. Each filterbank file (or
`\url{*fb*} file') may contain multiple buffers that may not be contiguous in
time. The time samples in each buffer are contiguous, but time continuity is
not guaranteed across buffers.


\subsubsection{Scripts}

The observation scripts are structured similar to the setup at Chilbolton. The
scripts are in \url{Scripts}. The top-level script is \url{FRBsearch.sh} which
runs the pipeline on all compute nodes. The compute node scripts are
\url{frb_abc0.sh}, \url{frb_abc1.sh}, \url{frb_abc2.sh}, and \url{frb_abc3.sh}.

The data acquisition pipeline is run only when ALFA is selected by the primary
observer. This is set up using cron. \url{cognizeALFA.rb} is run every minute
to check for the status of the receiver. If ALFA is enabled and observing
scripts are not, this script runs the observing scripts. If ALFA is disabled
and observing scripts are running, this script kills them. The pipeline is
restarted if the LO frequency changes. Running \url{crontab -e} will let one
view and edit the crontab.

The plotting script -- \url{generatePlots.sh} -- is run at 12:00 noon on the
day following a night of observations. This is set up using cron.

The following lists some of the major scripts in this directory:

\begin{itemize}
\item \url{alfabeams.py}: Displays the seven beams of ALFA and plots the
location of pulsar(s). Requires manual editing.
\item \url{extractBuffer.rb}: Similar to the Chilbolton script of similar name.
\item \url{generatePages.rb}: Generates web pages with plots.
\item \url{generatePlots.rb}: Copies latest data from compute nodes over to the
head node and runs the plotting Python script, \url{plotScatter.py}. This
script is called by \url{generatePlots.sh}, the cron script.
\item \url{getPointings.rb}: Get ALFA pointings from the SERENDIP VI (`S6' from
hereon) SCRAM dump.
\item \url{killobs}: Similar to the Chilbolton script of similar name.
\item \url{killobs_SSH}: Similar to the Chilbolton script of similar name.
\item \url{makeFil.py}: Script to convert a PCAP file to filterbank format.
Requires manual editing.
\item \url{removeBadLines.rb}: Removes corrupted lines in the \url{*dm*} files.
This is run in the plotting script.
\item \url{s6_redis_dump}: Script to dump some of the key-value pairs in the
Redis database on the S6 head node. For reference, mainly.
\item \url{taketcpdumpdata.sh}: Sample script to take PCAP data using tcpdump.
\end{itemize}



\section{Basic Operating Procedure}

The cron script \url{cognizeALFA.rb} starts and stops data acquisition
depending on the availability and configuration of ALFA. The restarting of data
acquisition happens quite often during AGES observing, as they spend a lot of
time calibrating their system, and keep changing the LO frequency. These data
-- usually a minute to a few minutes long -- are usually useless, as the LO
frequency may have changed during the observation. This does not happen much
with PALFA observations.

Observations at Arecibo usually start in late afternoon/evening and end
early/late morning. Daytime is usually reserved for maintenance. To reduce
the chance of conflict with ongoing AB data-taking, we run our plotting script
at mid-day, after the previous day's observing is over. If any data was
collected from 12:00 noon the previous day until 12:00 noon on the current day,
plots are generated for that data. Web pages are also generated, and these are
copied to the NAIC web server. These pages are available at
\url{naic.edu/~alfafrb}.

\subsection{Data Analysis}

Daily, manual monitoring of the generated plots is required. If an interesting
`blob' of data points is seen in a plot, the recommended procedure to follow is
given below.

We take as example, a plot generated on 7 September 2015, shown in
Figure~\ref{fig_dmvst}. This was a PALFA observing session, and as is typical
of such sessions, the observer observes a test pulsar first, in beam 0.
Ignoring the vertical streaks which are due to RFI, the test pulsar is the dark
blue (corresponding to beam 0) blob at a DM of about 100~cm$^{-3}$~pc near the
beginning of the observation. Towards the end of the observation, a set of
events is detected in beam 6 (red points), then in beam 5 (orange points), both
at a DM of about 250~cm$^{-3}$~pc. As we will see at the end of the following
steps, these sets of events are two single pulses of PSR B2002+31 when it
happened to be in the field of view, first in beam 6, and then in beam 5, as
the telescope slewed from one observing field to another\footnote{Although this
plot shows the power of the instrument in detecting transient events, it also
shows the difficulty involved in the manual inspection of plots, as there is no
quick way to differentiate between these two events and any of the RFI
events.}.

\begin{figure}[h]
\includegraphics[width=\textwidth]{AllBeams_D20150907T194704.png}
\caption{Diagnostic plot with a test pulsar in beam 0 and PSR B2002+31 in beams
    6 and 5.\label{fig_dmvst}}
\end{figure}

\subsubsection{Step 1: Get the buffer number}

First, check the \url{*dm*} file corresponding to the plot. SSH to \url{abc3},
corresponding to beam 6, and grep for the string `buffer' in the DM file.

\small{
\begin{verbatim}
$ grep buffer /data/Survey/Data/Beam6_dm_D20150907T194703.dat
...
# Written buffer :22 | MJDstart: 57273.04541088 | Best DM: 45 | Max SNR: 10.731744766235  Done
# Written buffer :23 | MJDstart: 57273.050023148 | Best DM: 1258 | Max SNR: 10.81814289093  Done
# Written buffer :24 | MJDstart: 57273.051967593 | Best DM: 262 | Max SNR: 13.327059745789  Done
# Written buffer :25 | MJDstart: 57273.052517361 | Best DM: 1227 | Max SNR: 10.498366355896  Done
# Written buffer :26 | MJDstart: 57273.06119213 | Best DM: 18 | Max SNR: 11.859872817993  Done
...
\end{verbatim}
}

We see that the best DM corresponding to buffer 24 matches what we see in the
plot. Examining this file in a text editor lets us see that the following is
the highest S/N event:

\small{
\begin{verbatim}
...
57273.052018887,   262, 13.327059745789, 32
...
\end{verbatim}
}

The event corresponds to a smoothing length of 32, which, at a sampling
interval of 256~$\mu$s, corresponds to a pulse width of 8.192~ms.


\subsubsection{Step 2: Extract the buffer}

The filterbank file \url{} contains all the buffers in which events were
detected in this observing session, so we will extract just the buffer we are
interested in.

\small{
\begin{verbatim}
$ cd /data/Survey/Tmp
$ ~/Survey/Scripts/extractBuffer.rb -b 24 ../Data/Beam6_fb_D20150907T194703.fil
...
$ ls -ltrh
...
-rw-rw-r-- 1 artemis artemis 65M Jun  2 14:04 Beam6_fb_D20150907T194703.buffer24.fil
\end{verbatim}
}

As shown, we first move to a scratch area (\url{/data/Survey/Tmp}), so as not
to pollute the \url{/data/Survey/Data}) directory with temporary files. The
extracted filterbank file contains continuous time samples that can be analysed
like a standard filterbank file. The nodes have both
SIGPROC\footnote{\url{http://sigproc.sourceforge.net/}} and
YAPP\footnote{\url{http://jayanthc.github.io/yapp/}} installed for this purpose.


\subsubsection{Step 3: Decimate the data}

Since the highest S/N event has a smoothing length of 32, we will decimate the
data in time, using SIGPROC (note that the output file size changes, as
expected).

\small{
\begin{verbatim}
$ decimate Beam6_fb_D20150907T194703.buffer24.fil -c 1 -t 32 > temp.fil
...
$ ls -ltrh
...
-rw-rw-r-- 1 artemis artemis 2.1M Jun  2 17:46 temp.fil
\end{verbatim}
}


\subsubsection{Step 4: Plot the buffer}

We will use YAPP to visualise the data.

\small{
\begin{verbatim}
$ yapp_viewdata -m hot -n 1024 temp.fil
\end{verbatim}
}

\begin{figure}[h]
\includegraphics[width=\textwidth]{fb_tsnofs.png}
\caption{Data smoothed/decimated in time by a factor of 32. A pulse can be made
    out between the 4- and 5-second marks.
    \label{fig_fb_tsnofs}}
\end{figure}

As you can see in the output in Figure~\ref{fig_fb_tsnofs}, the dispersed pulse
can be just about made out, between the 4- and 5-second marks. If the plot is
zoomed in (for instance, by skipping the first 4 seconds, and processing only 1
second of data), the pulse becomes clearer, as shown in
Figure~\ref{fig_fb_tsnofs_zoomed}.

\small{
\begin{verbatim}
$ yapp_viewdata -m hot -n 1024 -s 4 -p 1 temp.fil
\end{verbatim}
}

\begin{figure}[h]
\includegraphics[width=\textwidth]{fb_tsnofs_zoomed.png}
\caption{Same as in Figure~\ref{fig_fb_tsnofs}, but zoomed in.
    \label{fig_fb_tsnofs_zoomed}}
\end{figure}

In Figure~\ref{fig_fb_tsnofs_zoomed}, the dispersed pulse can be clearly seen.
Note that in some cases, the data may need to be decimated in frequency as
well, for the pulse to become apparent.


\subsubsection{Step 5: Get pointings}

...to be added...


\end{document}

